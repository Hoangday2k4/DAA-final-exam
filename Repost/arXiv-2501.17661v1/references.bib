@INPROCEEDINGS{fredrikssonExploration,
  title={Robotic Exploration through Semantic Topometric Mapping}, 
  author={Scott Fredriksson and Akshit Saradagi and George Nikolakopoulos},
  booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)}, 
  year={2024},
  volume={},
  number={},
  pages={9404-9410},
  }

@article{FREDRIKSSON_SEMANTIC_MAPPING,
title = {Semantic and Topological Mapping using Intersection Identification},
journal = {IFAC-PapersOnLine},
volume = {56},
number = {2},
pages = {9251-9256},
year = {2023},
note = {22nd IFAC World Congress},
issn = {2405-8963},
nodoi = {https://nodoi.org/10.1016/j.ifacol.2023.10.007},
noUrl = {https://www.sciencedirect.com/science/article/pii/S2405896323003415},
author = {Scott Fredriksson and Akshit Saradagi and George Nikolakopoulos},
keywords = {Semantics, Topological Mapping, Intersection Identification, Map building},
abstract = {This article presents a novel approach to identifying and classifying intersections for semantic and topological mapping. More specifically, the proposed novel approach has the merit of generating a semantically meaningful map containing intersections, pathways, dead ends, and pathways leading to unexplored frontiers. Furthermore, the resulting semantic map can be used to generate a sparse topological map representation, that can be utilized by robots for global navigation. The proposed solution also introduces a built-in filtering to handle noises in the environment, to remove openings in the map that the robot cannot pass, and to remove small objects to optimize and simplify the overall mapping results. The efficacy of the proposed semantic and topological mapping method is demonstrated over a map of an indoor structured environment that is built from experimental data. The proposed framework, when compared with similar state-of-the-art topological mapping solutions, is able to produce a map with up to 89% fewer nodes than the next best solution.}
}

@InProceedings{Garrote2017,
  author    = {Garrote, Luís and Rosa, José and Paulo, João and Premebida, Cristiano and Peixoto, Paulo and Nunes, Urbano J.},
  booktitle = {2017 {IEEE} {International} {Conference} on {Autonomous} {Robot} {Systems} and {Competitions} ({ICARSC})},
  title     = {{3D} point cloud downsampling for {2D} indoor scene modelling in mobile robotics},
  year      = {2017},
  month     = apr,
  pages     = {228--233},
  abstract  = {Sensory perception and environment modelling are important for autonomous navigation in mobile robotics. 2D discrete grid representations such as the classic 2D occupancy grid maps are a widely used technique in scene representation because of the inherent simplicity and compact representation. In recent years, many 2.5D and 3D grid-based methods have been proposed however, as for the 2D case, a compromise between keeping a low computational bound and reliable sensor interpretation must be kept in order to perform real-world tasks. Assuming the input data in the form of a 3D point-cloud, in this paper we propose a 2D scene modelling approach which converts the 3D data to a 2.5D representation and then to a 2D grid map in an efficient and meaningful manner. The proposed approach incorporates a new rapidly exploring random tree inspired ground-plane detection (RRT-GPD), and an inverse sensor model (ISM) to correctly map 3D to 2.5D and then to 2D grid cells. Experiments were conducted in indoor scenarios with a robotic walker platform equipped with a Microsoft's Kinect One and a LeddarTech's Leddar IS16 sensor. Reported results show an improvement on the representation of non-trivial obstacles (stairs, floor outlets) over a ROS package solution, when applied to a 3D point cloud input.},
  nodoi       = {10.1109/ICARSC.2017.7964080},
  file      = {:Garrote2017 - 3D Point Cloud Downsampling for 2D Indoor Scene Modelling in Mobile Robotics.pdf:PDF},
  groups    = {2.5D map},
  nourl       = {https://ieeexplore.ieee.org/abstract/document/7964080},
  nourldate   = {2023-09-27},
}
@Article{Hornung2013,
  author     = {Hornung, Armin and Wurm, Kai M. and Bennewitz, Maren and Stachniss, Cyrill and Burgard, Wolfram},
  journal    = {Autonomous Robots},
  title      = {{OctoMap}: an efficient probabilistic {3D} mapping framework based on octrees},
  year       = {2013},
  issn       = {1573-7527},
  month      = apr,
  number     = {3},
  pages      = {189--206},
  volume     = {34},
  abstract   = {Three-dimensional models provide a volumetric representation of space which is important for a variety of robotic applications including flying robots and robots that are equipped with manipulators. In this paper, we present an open-source framework to generate volumetric 3D environment models. Our mapping approach is based on octrees and uses probabilistic occupancy estimation. It explicitly represents not only occupied space, but also free and unknown areas. Furthermore, we propose an octree map compression method that keeps the 3D models compact. Our framework is available as an open-source C++ library and has already been successfully applied in several robotics projects. We present a series of experimental results carried out with real robots and on publicly available real-world datasets. The results demonstrate that our approach is able to update the representation efficiently and models the data consistently while keeping the memory requirement at a minimum.},
  nodoi        = {10.1007/s10514-012-9321-0},
  file       = {:Hornung2013 - OctoMap_ an Efficient Probabilistic 3D Mapping Framework Based on Octrees.html:nourl},
  groups     = {2.5D map},
  keywords   = {3D, Probabilistic, Mapping, Navigation},
  language   = {en},
  shorttitle = {{OctoMap}},
  nourl        = {https://nodoi.org/10.1007/s10514-012-9321-0},
  nourldate    = {2024-01-24},
}
@Article{Duberg2020,
  author     = {Duberg, Daniel and Jensfelt, Patric},
  journal    = {IEEE Robotics and Automation Letters},
  title      = {{UFOMap}: {An} {Efficient} {Probabilistic} {3D} {Mapping} {Framework} {That} {Embraces} the {Unknown}},
  year       = {2020},
  issn       = {2377-3766},
  month      = oct,
  number     = {4},
  pages      = {6411--6418},
  volume     = {5},
  abstract   = {3D models are an essential part of many robotic applications. In applications where the environment is unknown a-priori, or where only a part of the environment is known, it is important that the 3D model can handle the unknown space efficiently. Path planning, exploration, and reconstruction all fall into this category. In this letter we present an extension to OctoMap which we call UFOMap. UFOMap uses an explicit representation of all three states in the map, i.e., unknown, free, and occupied. This gives, surprisingly, a more memory efficient representation. We provide methods that allow for significantly faster insertions into the octree. Furthermore, UFOMap supports fast queries based on occupancy state using so called indicators and based on location by exploiting the octree structure and bounding volumes. This enables real-time colored octree mapping at high resolution (below 1 cm). UFOMap is contributed as a C++ library that can be used standalone but is also integrated into ROS.},
  nodoi        = {10.1109/LRA.2020.3013861},
  file       = {:Duberg2020 - UFOMap_ an Efficient Probabilistic 3D Mapping Framework That Embraces the Unknown.pdf:PDF},
  groups     = {2.5D map},
  shorttitle = {{UFOMap}},
  nourl        = {https://ieeexplore.ieee.org/document/9158399},
  nourldate    = {2023-10-20},
}
@InProceedings{Kamarudin2013,
  author    = {Kamarudin, Kamarulzaman and Mamduh, Syed Muhammad and Shakaff, Ali Yeon Md and Saad, Shaharil Mad and Zakaria, Ammar and Abdullah, Abu Hassan and Kamarudin, Latifah Munirah},
  booktitle = {2013 {IEEE} 9th {International} {Colloquium} on {Signal} {Processing} and its {Applications}},
  title     = {Method to convert {Kinect}'s {3D} depth data to a {2D} map for indoor {SLAM}},
  year      = {2013},
  month     = mar,
  pages     = {247--251},
  abstract  = {Mobile robotics has been strongly linked to localization and mapping especially for navigation purpose. A robot needs a sensor to see objects around it, avoid them and also map the surrounding area. The use of 1D and 2D proximity sensors such as ultrasonic sensor, sonar and laser range finder for area mapping is believed to be less effective since they do not provide information in Y or Z (horizontal and vertical) direction. The robot may miss an object due to its shape and position; thus increasing the risk of collision as well as inaccurate map. In this paper, a 3D visual device particularly Microsoft Kinect was used to perform area mapping. The 3D depth data from the device's depth sensor was retrieved and converted into 2D map using the presented method. A Graphical User Interface (GUI) was also implemented on the base station to depict the real-time map. It was found that the method applied has successfully mapped the potentially missing objects when using 1D or 2D sensor. The convincing results shown in this paper suggest that the Kinect is suitable for indoor SLAM application given that the device's limitations are solved.},
  nodoi       = {10.1109/CSPA.2013.6530050},
  file      = {:Kamarudin2013 - Method to Convert Kinect's 3D Depth Data to a 2D Map for Indoor SLAM.pdf:PDF},
  groups    = {2.5D map},
  keywords  = {Robot kinematics, Three-dimensional displays, Simultaneous localization and mapping, Base stations, Internet, Robotics, Microsoft Kinect, Navigation, Indoor SLAM, Image Processing},
  nourl       = {https://ieeexplore.ieee.org/abstract/document/6530050},
  nourldate   = {2024-02-14},
}
@InProceedings{Brahmanage2019,
  author    = {Brahmanage, Gayan and Leung, Henry},
  booktitle = {2019 {IEEE} {International} {Conference} on {Industrial} {Cyber} {Physical} {Systems} ({ICPS})},
  title     = {Building {2D} {Maps} with {Integrated} {3D} and {Visual} {Information} using {Kinect} {Sensor}},
  year      = {2019},
  month     = may,
  pages     = {218--223},
  abstract  = {RGB-D sensors can be used as a cost effective alternative to expensive laser scanners for small scale indoor mapping applications. Since RGB-D cameras provide both color and depth data, it is possible to fuse these two data frames in 2D mapping to exploit the advantages over laser scanners. An improved technique is to capture the navigation environment in an application for goal-oriented navigation and situation assessment using a mobile robot. The proposed technique is based on incorporating RGB images and 3D information to existing 2D Simultaneous Localization And Mapping (SLAM); which has not been clearly investigated before. By integrating visual and 3D information using an inexpensive RGB-D camera, the mobile robot is better equipped to perform tasks such as sign detection, object detection, and text understanding. The motivation for such an approach is to use 3D and visual information with reduced complexity. The performance of 2D mapping is exploited here by replacing the laser scanner with a RGB-D camera. In addition, memory usage is reduced by selecting key RGB-D frames from its sequence by comparing the number of overlapped RGB features. The proposed approach is evaluated for accuracy and consistency using experimental data gathered from a real environment.},
  nodoi       = {10.1109/ICPHYS.2019.8780288},
  file      = {:Brahmanage2019 - Building 2D Maps with Integrated 3D and Visual Information Using Kinect Sensor.pdf:PDF},
  groups    = {2.5D map},
  keywords  = {Three-dimensional displays, Visualization, Simultaneous localization and mapping, Two dimensional displays, Cameras},
  nourl       = {https://ieeexplore.ieee.org/document/8780288},
  nourldate   = {2024-02-14},
}

@INPROCEEDINGS{Multi_agent_Niklas,
  author={Dahlquist, Niklas and Lindqvist, Björn and Saradagi, Akshit and Nikolakopoulos, George},
  booktitle={2023 IEEE Conference on Control Technology and Applications (CCTA)}, 
  title={Reactive Multi-agent Coordination using Auction-based Task Allocation and Behavior Trees}, 
  year={2023},
  volume={},
  number={},
  pages={829-834},
  keywords={Costs;Robot kinematics;MIMICs;Computer architecture;Switches;Production facilities;Behavioral sciences},
  doi={10.1109/CCTA54093.2023.10252961}}

@Article{Yang2018,
  author   = {Yang, Sining and Yang, Shaowu and Yi, Xiaodong},
  journal  = {IEEE Access},
  title    = {An {Efficient} {Spatial} {Representation} for {Path} {Planning} of {Ground} {Robots} in {3D} {Environments}},
  year     = {2018},
  issn     = {2169-3536},
  pages    = {41539--41550},
  volume   = {6},
  abstract = {For efficient path planning of ground robots in 3D environments with structures such as buildings or overhanging objects, an appropriate spatial representation of the environment is normally required. Some popular representations, such as elevation maps and multi-level surface maps, need to be projected into a 2D plane to extract traversibility maps for path planning. They cannot properly handle all complex situations, such as bridges. Some other predominant representations, such as 3D occupancy grid maps and 3D normal distributions maps, typically have high computational and storage demands. In this paper, we propose a 2.5D normal distributions transform map (NDT map) as an efficient and compact representation of 3D environments for path planning of ground robots. Our open-source work partitions the space evenly in x - y direction and z direction separately and transforms the 3D point clouds of environments into 2.5D representation based on the NDT. The 2.5D-NDT map only stores space surface patches that are potentially navigable for path planning of ground robots, and represents them with four parameters based on the NDT. Moreover, the map is efficiently organized by our proposed two-layer indexes to speed up the computation. We further present algorithms for a traversability analysis and path planning, which utilize the proposed map. Experiments on data sets, containing indoor and outdoor scenarios, demonstrate that our approach can represent 3D environments properly and compactly for path planning of ground robots. Paths suitable for navigation of ground robots can be planned efficiently in complex 3D environments based on our proposed algorithm.},
  nodoi      = {10.1109/ACCESS.2018.2858809},
  file     = {:Yang2018 - An Efficient Spatial Representation for Path Planning of Ground Robots in 3D Environments.pdf:PDF},
  groups   = {2.5D map},
  keywords = {Three-dimensional displays, Robots, Path planning, Gaussian distribution, Indexes, Transforms, Two dimensional displays, 25D-NDT maps, 3D spatial representation, ground robots, path planning},
  nourl      = {https://ieeexplore.ieee.org/abstract/document/8418378},
  nourldate  = {2024-03-11},
}
@Article{Li2024,
  author    = {Li, Yiduo and Wang, Debao and Li, Qipeng and Cheng, Guangtao and Li, Zhuoran and Li, Peiqing},
  journal   = {Electronics},
  title     = {Advanced {3D} {Navigation} {System} for {AGV} in {Complex} {Smart} {Factory} {Environments}},
  year      = {2024},
  issn      = {2079-9292},
  month     = jan,
  number    = {1},
  pages     = {130},
  volume    = {13},
  abstract  = {The advancement of Industry 4.0 has significantly propelled the widespread application of automated guided vehicle (AGV) systems within smart factories. As the structural diversity and complexity of smart factories escalate, the conventional two-dimensional plan-based navigation systems with fixed routes have become inadequate. Addressing this challenge, we devised a novel mobile robot navigation system encompassing foundational control, map construction positioning, and autonomous navigation functionalities. Initially, employing point cloud matching algorithms facilitated the construction of a three-dimensional point cloud map within indoor environments, subsequently converted into a navigational two-dimensional grid map. Simultaneously, the utilization of a multi-threaded normal distribution transform (NDT) algorithm enabled precise robot localization in three-dimensional settings. Leveraging grid maps and the robot’s inherent localization data, the A* algorithm was utilized for global path planning. Moreover, building upon the global path, the timed elastic band (TEB) algorithm was employed to establish a kinematic model, crucial for local obstacle avoidance planning. This research substantiated its findings through simulated experiments and real vehicle deployments: Mobile robots scanned environmental data via laser radar and constructing point clouds and grid maps. This facilitated centimeter-level localization and successful circumvention of static obstacles, while simultaneously charting optimal paths to bypass dynamic hindrances. The devised navigation system demonstrated commendable autonomous navigation capabilities. Experimental evidence showcased satisfactory accuracy in practical applications, with positioning errors of 3.6 cm along the x-axis, 3.3 cm along the y-axis, and 4.3° in orientation. This innovation stands to substantially alleviate the low navigation precision and sluggishness encountered by AGV vehicles within intricate smart factory environments, promising a favorable prospect for practical applications.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  nodoi       = {10.3390/electronics13010130},
  file      = {:Li2024 - Advanced 3D Navigation System for AGV in Complex Smart Factory Environments.pdf:PDF},
  groups    = {2.5D map},
  keywords  = {AGV, navigation, mapping and location, path planning, smart factory},
  language  = {en},
  publisher = {Multidisciplinary Digital Publishing Institute},
  nourl       = {https://www.mdpi.com/2079-9292/13/1/130},
  nourldate   = {2024-03-12},
}
@InProceedings{Gim2021,
  author    = {Gim, Haeyeon and Jeong, Minwook and Han, Soohee},
  booktitle = {2021 21st {International} {Conference} on {Control}, {Automation} and {Systems} ({ICCAS})},
  title     = {Autonomous {Navigation} {System} with {Obstacle} {Avoidance} using 2.{5D} {Map} {Generated} by {Point} {Cloud}},
  year      = {2021},
  month     = oct,
  note      = {ISSN: 2642-3901},
  pages     = {749--752},
  abstract  = {The development of a robust obstacle avoidance system for the autonomous mobile robot has become important as their applications become widespread. Even though plenty of autonomous systems have been developed, safe driving in complex environments, such as crowded places or a place with many obstacles in the path, is still a challenging task. In this paper, we propose an autonomous navigation system of a mobile robot in a dynamic environment. We build a 2.5D map, that integrates a 2D grid map with dynamic objects 3D geometry information. From the 3D LiDAR point cloud, dynamic points are detected by tracking the occupancy changes over time. Then remained static points are used for generating a 2D grid map by SLAM algorithm. A computation cost is reduced efficiently by reconstructing only the necessary parts for the mobile robot driving into the high resolution of raw point cloud data. In addition, it can be easily utilized in the embedded board since the system does not need any complex calculations. Our experiments demonstrate that the system is possible to simultaneously perform multiple roles of map building, localization, and dynamic obstacle avoidance using real-time incoming 3D point cloud data.},
  nodoi       = {10.23919/ICCAS52745.2021.9649862},
  file      = {:Gim2021 - Autonomous Navigation System with Obstacle Avoidance Using 2.5D Map Generated by Point Cloud.html:nourl},
  groups    = {2.5D map},
  issn      = {2642-3901},
  keywords  = {Point cloud compression, Three-dimensional displays, Costs, Laser radar, Simultaneous localization and mapping, Search problems, Windows, Autonomous mobile robot, navigation system, dynamic detection, obstacle avoidance, 2.5D map, point cloud, application},
  nourl       = {https://ieeexplore.ieee.org/document/9649862},
  nourldate   = {2024-03-13},
}
@Article{Megalingam2023,
  author   = {Megalingam, Rajesh Kannan and Tantravahi, Santosh and Tammana, Hemanth Sai Surya Kumar and Puram, Hari Sudarshan Rahul},
  journal  = {International Journal of Intelligent Robotics and Applications},
  title    = {{2D}-{3D} hybrid mapping for path planning in autonomous robots},
  year     = {2023},
  issn     = {2366-598X},
  month    = jun,
  number   = {2},
  pages    = {291--303},
  volume   = {7},
  abstract = {Computational complexity is one of the critical attributes of robot design. Mapping, a vitally important feature of auto-navigation of robots is one such area where computational complexity is of concern. An appropriate spatial representation of the surroundings is required for efficient path planning. Creation of a 2D map alone for a given environment is not sufficient even though it has the least computational complexity. 3D occupancy grid maps and 3D normal distribution maps need larger space and time. This research proposes an efficient hybrid mapping technique (2D-3Dh) which uses both 2D and 3D sensor data mapping to represent an efficient and compact representation of the environments aiming at reducing the computational complexity. The 2D maps are built from the laser data obtained from the Light Detection and Ranging (LiDAR) sensor and 3D maps are built from the data obtained from the Kinect sensor. The proposed algorithm efficiently organizes the 2D-3Dh map in such a way that the mapping switches from 2 to 3D only when an elevation region is detected in the environment. The newly generated map consists of both 2D and 3D data, with optimized mapping time and storage capacity requirements. Simulation results indicate that the proposed hybrid mapping can save mapping time up to 46.7\% and storage space up to 98.5\%. Real time experiments show that 2D-3Dh mapping can save more than 33\% of memory space and requires less than half the time compared to that of full 3D mapping.},
  nodoi      = {10.1007/s41315-023-00272-4},
  file     = {Full Text PDF:https\://link.springer.com/content/pdf/10.1007%2Fs41315-023-00272-4.pdf:application/pdf},
  groups   = {2.5D map},
  keywords = {Mapping, Path planning, Particle filter, Computational complexity},
  language = {en},
  nourl      = {https://nodoi.org/10.1007/s41315-023-00272-4},
  nourldate  = {2024-03-14},
}
@Article{Yusefi2020,
  author    = {Yusefi, Abdullah and Durdu, Akif and Sungur, Cemil},
  journal   = {Avrupa Bilim ve Teknoloji Dergisi},
  title     = {{ORB}-{SLAM}-based {2D} {Reconstruction} of {Environment} for {Indoor} {Autonomous} {Navigation} of {UAVs}},
  year      = {2020},
  issn      = {2148-2683},
  month     = oct,
  pages     = {466--472},
  abstract  = {In this paper, a simple and economic yet efficient autonomous mapping and navigation system for unmanned aerial vehicles is presented. In order to realize this system, three modules have been implemented. First module constructs a 3D model of the environment while autonomously navigating the drone and is based on one of the top monocular SLAM algorithms called ORB-SLAM. For the autonomous navigation of the system a visual-based line tracking method is proposed. Afterwards, the second module performs a real time transformation of the 3D map to 2D grid map. While most of the 3D to 2D map conversion studies use octomaps in the middle of two, we present a threshold-based method that directly converts the 3D map to 2D without need for any middle component. Finally, third module uses A* path planning algorithm to navigate the drone to the goal pose in the constructed 2D grid map. This module uses only IMU-aided Adaptive Monte Carlo localization (AMCL) combined with monocular camera information to complete this task. The experimentation results indicate that the proposed system is adequately efficient to be used in the low-cost drones that have only a monocular camera and limited processing resources on them.},
  nodoi       = {10.31590/ejosat.819620},
  file      = {:Yusefi2020 - ORB SLAM Based 2D Reconstruction of Environment for Indoor Autonomous Navigation of UAVs.pdf:PDF},
  groups    = {2.5D map},
  language  = {en},
  publisher = {Osman SAĞDIÇ},
  nourl       = {https://dergipark.org.tr/en/pub/ejosat/issue/56959/819620},
  nourldate   = {2024-03-14},
}
@Article{MurArtal2015,
  author     = {Mur-Artal, Raúl and Montiel, J. M. M. and Tardós, Juan D.},
  journal    = {IEEE Transactions on Robotics},
  title      = {{ORB}-{SLAM}: {A} {Versatile} and {Accurate} {Monocular} {SLAM} {System}},
  year       = {2015},
  issn       = {1941-0468},
  month      = oct,
  number     = {5},
  pages      = {1147--1163},
  volume     = {31},
  abstract   = {This paper presents ORB-SLAM, a feature-based monocular simultaneous localization and mapping (SLAM) system that operates in real time, in small and large indoor and outdoor environments. The system is robust to severe motion clutter, allows wide baseline loop closing and relocalization, and includes full automatic initialization. Building on excellent algorithms of recent years, we designed from scratch a novel system that uses the same features for all SLAM tasks: tracking, mapping, relocalization, and loop closing. A survival of the fittest strategy that selects the points and keyframes of the reconstruction leads to excellent robustness and generates a compact and trackable map that only grows if the scene content changes, allowing lifelong operation. We present an exhaustive evaluation in 27 sequences from the most popular datasets. ORB-SLAM achieves unprecedented performance with respect to other state-of-the-art monocular SLAM approaches. For the benefit of the community, we make the source code public.},
  nodoi        = {10.1109/TRO.2015.2463671},
  file       = {IEEE Xplore Full Text PDF:https\://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=7219438&ref=aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50LzcyMTk0Mzg/Y2FzYV90b2tlbj01Y0FESjVsQUk4TUFBQUFBOkVVX3hFRllIUTMxc0J6aWxjamFWVS1iNFc3V2pWZkMycWRFSjQyS3VRZTBIMDhVcXZSSk0xbFl0cUFoRVRMXzBGTTduYkc4TXF3:application/pdf},
  groups     = {2.5D map},
  keywords   = {Simultaneous localization and mapping, Cameras, Optimization, Feature extraction, Visualization, Real-time systems, Computational modeling, Lifelong mapping, localization, monocular vision, recognition, simultaneous localization and mapping (SLAM), Lifelong mapping, localization, monocular vision, recognition, simultaneous localization and mapping (SLAM)},
  shorttitle = {{ORB}-{SLAM}},
  nourl        = {https://ieeexplore.ieee.org/abstract/document/7219438?casa_token=5cADJ5lAI8MAAAAA:EU_xEFYHQ31sBzilcjaVU-b4W7WjVfC2qdEJ42KuQe0H08UqvRJM1lYtqAhETL_0FM7nbG8Mqw},
  nourldate    = {2024-03-14},
}
@InProceedings{Wulf2004,
  author    = {Wulf, O. and Arras, K.O. and Christensen, H.I. and Wagner, B.},
  booktitle = {{IEEE} {International} {Conference} on {Robotics} and {Automation}, 2004. {Proceedings}. {ICRA} '04. 2004},
  title     = {{2D} mapping of cluttered indoor environments by means of {3D} perception},
  year      = {2004},
  month     = apr,
  note      = {ISSN: 1050-4729},
  pages     = {4204--4209 Vol.4},
  volume    = {4},
  abstract  = {This paper presents a combination of a 3D laser sensor and a line-base SLAM algorithm which together produce 2D line maps of highly cluttered indoor environments. The key of the described method is the replacement of commonly used 2D laser range sensors by 3D perception. A straightforward algorithm extracts a virtual 2D scan that also contains partially occluded walls. These virtual scans are used as input for SLAM using line segments as features. The paper presents the used algorithms and experimental results that were made in a former industrial bakery. The focus lies on scenes that are known to be problematic for pure 2D systems. The results demonstrate that mapping indoor environments can be made robust with respect to both, poor odometry and clutter.},
  nodoi       = {10.1109/ROBOT.2004.1308934},
  groups    = {2.5D map},
  issn      = {1050-4729},
  keywords  = {Indoor environments, Navigation, Sensor phenomena and characterization, Simultaneous localization and mapping, Robot sensing systems, Sensor systems, Mobile robots, Layout, Content addressable storage, Systems engineering and theory},
  nourl       = {https://ieeexplore.ieee.org/abstract/document/1308934?casa_token=XlWYjDYXQ3oAAAAA:iKAyqDGNWLN8hcVASfUdkdOQggUuT7LCYTrEVscQ8nLzzzvDEpVjLboNXmJz95iZlTckjkdIiw},
  nourldate   = {2024-03-14},
}
@Article{Mora2023,
  author   = {Mora, Alicia and Barber, Ramon and Moreno, Luis},
  journal  = {IEEE Sensors Journal},
  title    = {Leveraging {3D} {Data} for {Whole} {Object} {Shape} and {Reflection} {Aware} {2D} {Map} {Building}},
  year     = {2023},
  issn     = {1558-1748},
  pages    = {1--1},
  abstract = {Two-dimensional laser scan sensors stand out as the preferred choice for robot mapping applications. However, these sensors have a significant drawback. Encountering objects with varying shapes at different heights, such as tables, poses challenges for these sensors due to their limited detection capability resulting from their dimensionality. This limitation increases the risk of potential collisions. Additionally, there are multiple polished materials that generate noise due to reflection. In order to have a robust occupancy grid map representation, these problems must be addressed. This paper proposes the usage of a 3D laser scan sensor to generate a 2D occupancy grid map that incorporates the complete geometry of objects and effectively filters out noise from reflective materials like glass. The main novelty of the method is that it takes advantage of all the available 3D data to avoid any information loss about objects’ shapes. Additionally, a new approach for filtering reflection noise based on the analysis of indoor structural elements is proposed. Both approaches are merged for the creation of a robust indoor representation that allows to safely navigate the environment. Finally, a recursive Bayesian filter is applied for merging data, so noise due to dynamic elements that appeared during data collection is also filtered. Experimental evaluations in indoor environments with diverse objects and reflective surfaces, including dynamic elements like people, demonstrate the effectiveness of the proposed approach.},
  nodoi      = {10.1109/JSEN.2023.3321936},
  file     = {IEEE Xplore Full Text PDF:https\://ieeexplore.ieee.org/ielx7/7361/4427201/10287264.pdf?tp=&arnumber=10287264&isnumber=4427201&ref=aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50LzEwMjg3MjY0:application/pdf},
  groups   = {2.5D map},
  keywords = {Sensors, Three-dimensional displays, Robots, Robot sensing systems, Measurement by laser beam, Robot kinematics, Shape, Robust Map Building, Occupancy Grid Map, 3D Laser Scan, Reflection},
  nourl      = {https://ieeexplore.ieee.org/abstract/document/10287264},
  nourldate  = {2024-03-14},
}
@Article{Nam2017,
  author    = {Nam, Tae Hyeon and Shim, Jae Hong and Cho, Young Im},
  journal   = {Sensors},
  title     = {A 2.{5D} {Map}-{Based} {Mobile} {Robot} {Localization} via {Cooperation} of {Aerial} and {Ground} {Robots}},
  year      = {2017},
  issn      = {1424-8220},
  month     = dec,
  number    = {12},
  pages     = {2730},
  volume    = {17},
  abstract  = {Recently, there has been increasing interest in studying the task coordination of aerial and ground robots. When a robot begins navigation in an unknown area, it has no information about the surrounding environment. Accordingly, for robots to perform tasks based on location information, they need a simultaneous localization and mapping (SLAM) process that uses sensor information to draw a map of the environment, while simultaneously estimating the current location of the robot on the map. This paper aims to present a localization method based in cooperation between aerial and ground robots in an indoor environment. The proposed method allows a ground robot to reach accurate destination by using a 2.5D elevation map built by a low-cost RGB-D (Red Green and Blue-Depth) sensor and 2D Laser sensor attached onto an aerial robot. A 2.5D elevation map is formed by projecting height information of an obstacle using depth information obtained by the RGB-D sensor onto a grid map, which is generated by using the 2D Laser sensor and scan matching. Experimental results demonstrate the effectiveness of the proposed method for its accuracy in location recognition and computing speed.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  nodoi       = {10.3390/s17122730},
  file      = {Full Text PDF:https\://www.mdpi.com/1424-8220/17/12/2730/pdf?version=1512022301:application/pdf},
  groups    = {2.5D map},
  keywords  = {SLAM, localization, ground robot, aerial robot, cooperation, low cost sensor, indoor},
  language  = {en},
  publisher = {Multidisciplinary Digital Publishing Institute},
  nourl       = {https://www.mdpi.com/1424-8220/17/12/2730},
  nourldate   = {2024-03-14},
}
@InProceedings{Sandfuchs2021,
  author    = {Sandfuchs, Stephan and Heimbach, Moritz P. and Weber, Jan and Schmidt, Marco},
  booktitle = {2021 {European} {Conference} on {Mobile} {Robots} ({ECMR})},
  title     = {Conversion of depth images into planar laserscans considering obstacle height for collision free {2D} robot navigation},
  year      = {2021},
  month     = aug,
  pages     = {1--6},
  abstract  = {Mobile robots have become popular in many application areas over the last few decades. In order to perceive their environment in which they move autonomously, various sensors such as depth cameras are used. The processing of 3D information from depth images is very computationally expensive due to the large amount of data. However, for many mobile robots, navigation in a simplified 2D world is sufficient. For this purpose, the depth images of the environment can first be reduced to 2D information in the form of a laserscan line and then processed with algorithms for localization and mapping. This paper improves an existing algorithm that converts depth images into laserscans and tests it on a real robot in a real world scenario. In contrast to the original algorithm, the improved algorithm considers 3D information such as the height of the robot and obstacles when creating the laserscan line. This allows a mobile robot to navigate in a simplified 2D world without colliding with obstacles in the real 3D world. Since processing the 3D information is computationally expensive, the algorithm was optimized to be executable on low-cost single-board computers.},
  nodoi       = {10.1109/ECMR50962.2021.9568795},
  file      = {:Sandfuchs2021 - Conversion of Depth Images into Planar Laserscans Considering Obstacle Height for Collision Free 2D Robot Navigation.html:URL},
  groups    = {2.5D map},
  keywords  = {Location awareness, Image sensors, Three-dimensional displays, Laser radar, Navigation, Two dimensional displays, Robot vision systems},
  nourl       = {https://ieeexplore.ieee.org/document/9568795},
  urldate   = {2024-03-13},
}
@Article{Remolina2004,
  author       = {Remolina, Emilio and Kuipers, Benjamin},
  title        = {{Towards a general theory of topological maps}},
  issn         = {0004-3702},
  number       = {1},
  pages        = {47--104},
  volume       = {152},
  abstract     = {We present a general theory of topological maps whereby sensory input, topological and local metrical information are combined to define the topological maps explaining such information. Topological maps correspond to the minimal models of an axiomatic theory describing the relationships between the different sources of information explained by a map. We use a circumscriptive theory to specify the minimal models associated with this representation. The theory here proposed is independent of the exploration strategy the agent follows when building a map. We provide an algorithm to calculate the models of the theory. This algorithm supports different exploration strategies and facilitates map disambiguation when perceptual aliasing arises.},
  date         = {2004},
  doi          = {https://doi.org/10.1016/S0004-3702(03)00114-0},
  groups       = {GRID-FAST, Exploration with top map},
  journaltitle = {Artificial Intelligence},
  keywords     = {Causal maps,Cognitive robotics,Map building,Nested abnormality theories (NATs),Spatial representations,Spatial semantic hierarchy (SSH),Topological maps,View graph},
}
@Article{Kostavelis2015,
  author       = {Kostavelis, Ioannis and Gasteratos, Antonios},
  title        = {{Semantic mapping for mobile robotics tasks: A survey}},
  issn         = {0921-8890},
  pages        = {86--103},
  volume       = {66},
  abstract     = {The evolution of contemporary mobile robotics has given thrust to a series of additional conjunct technologies. Of such is the semantic mapping, which provides an abstraction of space and a means for human–robot communication. The recent introduction and evolution of semantic mapping motivated this survey, in which an explicit analysis of the existing methods is sought. The several algorithms are categorized according to their primary characteristics, namely scalability, inference model, temporal coherence and topological map usage. The applications involving semantic maps are also outlined in the work at hand, emphasizing on human interaction, knowledge representation and planning. The existence of publicly available validation datasets and benchmarking, suitable for the evaluation of semantic mapping techniques is also discussed in detail. Last, an attempt to address open issues and questions is also made.},
  date         = {2015},
  doi          = {https://doi.org/10.1016/j.robot.2014.12.006},
  groups       = {GRID-FAST, Exploration with top map},
  journaltitle = {Robotics and Autonomous Systems},
  keywords     = {Human–robot interaction,Knowledge representation,Mobile robots,Object recognition,Place recognition,Planning,Semantic map,Temporal coherence,Topological map},
  nourl          = {https://www.sciencedirect.com/science/article/pii/S0921889014003030},
}

@article{sharon2015conflict,
  title={Conflict-based search for optimal multi-agent pathfinding},
  author={Sharon, Guni and Stern, Roni and Felner, Ariel and Sturtevant, Nathan R},
  journal={Artificial intelligence},
  volume={219},
  pages={40--66},
  year={2015},
  publisher={Elsevier}
}

@article{yu2015optimal,
  title={Optimal multi-robot path planning on graphs: Structure and computational complexity},
  author={Yu, Jingjin and LaValle, Steven M},
  journal={arXiv preprint arXiv:1507.03289},
  year={2015}
}

@article{honig2018trajectory,
  title={Trajectory planning for quadrotor swarms},
  author={H{\"o}nig, Wolfgang and Preiss, James A and Kumar, TK Satish and Sukhatme, Gaurav S and Ayanian, Nora},
  journal={IEEE Transactions on Robotics},
  volume={34},
  number={4},
  pages={856--869},
  year={2018},
  publisher={IEEE}
}

@inproceedings{ma2019lifelong,
  title={Lifelong path planning with kinematic constraints for multi-agent pickup and delivery},
  author={Ma, Hang and H{\"o}nig, Wolfgang and Kumar, TK Satish and Ayanian, Nora and Koenig, Sven},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={7651--7658},
  year={2019}
}

@inproceedings{stern2019multi,
  title={Multi-agent pathfinding: Definitions, variants, and benchmarks},
  author={Stern, Roni and Sturtevant, Nathan and Felner, Ariel and Koenig, Sven and Ma, Hang and Walker, Thayne and Li, Jiaoyang and Atzmon, Dor and Cohen, Liron and Kumar, TK and others},
  booktitle={Proceedings of the International Symposium on Combinatorial Search},
  volume={10},
  number={1},
  pages={151--158},
  year={2019}
}

@inproceedings{ma2019searching,
  title={Searching with consistent prioritization for multi-agent path finding},
  author={Ma, Hang and Harabor, Daniel and Stuckey, Peter J and Li, Jiaoyang and Koenig, Sven},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={33},
  number={01},
  pages={7643--7650},
  year={2019}
}

@inproceedings{barer2014suboptimal,
  title={Suboptimal variants of the conflict-based search algorithm for the multi-agent pathfinding problem},
  author={Barer, Max and Sharon, Guni and Stern, Roni and Felner, Ariel},
  booktitle={Proceedings of the international symposium on combinatorial Search},
  volume={5},
  number={1},
  pages={19--27},
  year={2014}
}

@inproceedings{ma2021distributed,
  title={Distributed heuristic multi-agent path finding with communication},
  author={Ma, Ziyuan and Luo, Yudong and Ma, Hang},
  booktitle={2021 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={8699--8705},
  year={2021},
  organization={IEEE}
}

@article{lafmejani2021nonlinear,
  title={Nonlinear MPC for collision-free and deadlock-free navigation of multiple nonholonomic mobile robots},
  author={Lafmejani, Amir Salimi and Berman, Spring},
  journal={Robotics and Autonomous Systems},
  volume={141},
  pages={103774},
  year={2021},
  publisher={Elsevier}
}

@article{park2023dlsc,
  title={Dlsc: Distributed multi-agent trajectory planning in maze-like dynamic environments using linear safe corridor},
  author={Park, Jungwon and Lee, Yunwoo and Jang, Inkyu and Kim, H Jin},
  journal={IEEE Transactions on Robotics},
  volume={39},
  number={5},
  pages={3739--3758},
  year={2023},
  publisher={IEEE}
}

@inproceedings{standley2010finding,
  title={Finding optimal solutions to cooperative pathfinding problems},
  author={Standley, Trevor},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={24},
  number={1},
  pages={173--178},
  year={2010}
}

@article{wagner2015subdimensional,
  title={Subdimensional expansion for multirobot path planning},
  author={Wagner, Glenn and Choset, Howie},
  journal={Artificial intelligence},
  volume={219},
  pages={1--24},
  year={2015},
  publisher={Elsevier}
}

@article{li2021pairwise,
  title={Pairwise symmetry reasoning for multi-agent path finding search},
  author={Li, Jiaoyang and Harabor, Daniel and Stuckey, Peter J and Ma, Hang and Gange, Graeme and Koenig, Sven},
  journal={Artificial Intelligence},
  volume={301},
  pages={103574},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{li2019symmetry,
  title={Symmetry-breaking constraints for grid-based multi-agent path finding},
  author={Li, Jiaoyang and Harabor, Daniel and Stuckey, Peter J and Ma, Hang and Koenig, Sven},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={33},
  number={01},
  pages={6087--6095},
  year={2019}
}

@inproceedings{boyarski2015icbs,
  title={Icbs: The improved conflict-based search algorithm for multi-agent pathfinding},
  author={Boyarski, Eli and Felner, Ariel and Stern, Roni and Sharon, Guni and Betzalel, Oded and Tolpin, David and Shimony, Eyal},
  booktitle={Proceedings of the International Symposium on Combinatorial Search},
  volume={6},
  number={1},
  pages={223--225},
  year={2015}
}

@inproceedings{gange2019lazy,
  title={Lazy CBS: implicit conflict-based search using lazy clause generation},
  author={Gange, Graeme and Harabor, Daniel and Stuckey, Peter J},
  booktitle={Proceedings of the international conference on automated planning and scheduling},
  volume={29},
  pages={155--162},
  year={2019}
}

@inproceedings{tajbakhsh2024conflict,
  title={Conflict-based model predictive control for scalable multi-robot motion planning},
  author={Tajbakhsh, Ardalan and Biegler, Lorenz T and Johnson, Aaron M},
  booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={14562--14568},
  year={2024},
  organization={IEEE}
}

@inproceedings{okumura2023lacam,
  title={Lacam: Search-based algorithm for quick multi-agent pathfinding},
  author={Okumura, Keisuke},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={10},
  pages={11655--11662},
  year={2023}
}

@INPROCEEDINGS{Hector2011,
  author = {S. Kohlbrecher and J. Meyer and O. von Stryk and U. Klingauf},
  title = {A Flexible and Scalable SLAM System with Full 3D Motion Estimation},
  year = {2011},
  month = {November},
  booktitle = {Proc. IEEE International Symposium on Safety, Security and Rescue Robotics (SSRR)},
  organization = {IEEE},
}

@article{mcbeth2023scalable,
  title={Scalable Multi-robot Motion Planning for Congested Environments With Topological Guidance},
  author={McBeth, Courtney and Motes, James and Uwacu, Diane and Morales, Marco and Amato, Nancy M},
  journal={IEEE Robotics and Automation Letters},
  year={2023},
  publisher={IEEE}
}

@article{via2020efficient,
  title={Efficient Trajectory Planning for Multiple Non-holonomic Mobile Robots via Prioritized Trajectory Optimization},
  author={via Prioritized, Non-holonomic Mobile Robots},
  year={2020}
}